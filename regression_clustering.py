# -*- coding: utf-8 -*-
"""Regression_Clustering

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ytYH7l4_YErX2_ThvM-mtOoAjSU9laVM

# Импорт необходимых пакетов
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# sklearn packages
import sklearn.metrics as mtrx # меры качества
from sklearn.preprocessing import MinMaxScaler # масштабирование
from sklearn.model_selection import GridSearchCV # Поиск по решетке
from sklearn.model_selection import cross_val_score # оценка кросс-валидации
from sklearn.model_selection import cross_validate # кросс-валидация
from sklearn.model_selection import train_test_split # разделение на обучающую/тестовую выборки

from sklearn.svm import SVR # метод регрессии опорных векторов
from sklearn.naive_bayes import GaussianNB # наивный Байес

from scipy.cluster.hierarchy import dendrogram # дендрограмма
from sklearn.cluster import AgglomerativeClustering # агломеративная кластеризация
from sklearn.cluster import KMeans # к-средних

import warnings
warnings.filterwarnings('ignore')

"""# Задача регрессии

Смоделируем тестовую задачу регрессии.

$ y = sin(x) + N(0, 0.25), x \in [-5,9] $
"""

# sample volume
volume = 50
minX = -3
maxX = 9

x = np.random.uniform(minX, maxX, volume)
y = np.sin(x)
yNoisy = y + np.random.normal(0, 0.25, volume)

# сохраним в датафрейм как зашумеленные, так и исходные данные
data = pd.DataFrame({'x':x, 'y': yNoisy, 'yClean': y})
data.head()

"""Построим график по выборке"""

xSin = np.linspace(minX, maxX, 200)
ySin = np.sin(xSin)

fig, ax = plt.subplots(figsize=(12,7))
ax.plot(xSin, ySin, color='red', label='Истинная зависимость')
ax.scatter(data['x'], data['y'], color='blue', label='данные для анализа')
ax.legend()
plt.show()

"""Проверим диапазоны"""

data[['x','y']].describe()

"""В многомерных данных, масштабы значенй переменных могут существенно отличаться - необходимо привести к единому.

Для многих методов необходимо чтобы данные были в конкретном диапазоне (например, $[0,1]$).
"""

# мастабировние в [0,1]
scaler1 = MinMaxScaler()
tmp = scaler1.fit_transform(data[['x', 'y']])

dataScaled = pd.DataFrame(tmp, columns=['x','y'])
dataScaled.head()

fig, ax = plt.subplots(ncols=2, figsize=(16,5))
ax[0].scatter(dataScaled['x'],dataScaled['y'],
           color='blue')
ax[0].set_title("с масштабированием")
ax[1].scatter(data['x'],data['y'],
           color='blue')
ax[1].set_title("без масштабирования")
plt.show()

"""Для валидации разделим данные на обучающую и тестовую выборки"""

# split into train and test
# trainX, testX, trainY, testY = train_test_split(X, Y)
train, test = train_test_split(dataScaled)
print("фрагмент обучающей выборки")
print(train.head(3))
print("")
print("фрагмент тестовой выборки")
print(test.head(3))

"""Построим модель методом регрессии опорных векторов"""

# train SVR model with some parameters
svrModel = SVR(C=1.0, epsilon=0.2)
svrModel.fit(train['x'].values.reshape(-1,1),
             train['y'].values.reshape(-1,1))
predictSVR = svrModel.predict(test['x'].values.reshape(-1,1))

"""Построим кривую по полученной модели"""

# draw SVR model curve
xSVR = np.linspace(0, 1, 200)
ySVR = svrModel.predict(xSVR.reshape(-1,1))

fig, ax = plt.subplots(figsize=(12,7))
ax.plot(xSVR,ySVR,color='red',
        label='Модель SVR')
ax.scatter(dataScaled['x'],dataScaled['y'],
           color='blue', label='данные')
ax.legend()
plt.show()

"""Вычислим некоторые меры качества"""

# some regresion metrics
print('MSE:', mtrx.mean_squared_error(predictSVR, test['y']))
print('MSE (root):',
      mtrx.mean_squared_error(predictSVR, test['y'],
                              squared=False))
print('MAE:', mtrx.mean_absolute_error(predictSVR, test['y']))
print('R2:', mtrx.r2_score(predictSVR, test['y']))

# plot test data and prediction
fig, ax = plt.subplots(figsize=(12,7))
ax.scatter(test['x'],test['y'],
           color='red', label='данные тестового набора')
ax.scatter(test['x'],predictSVR,
           color='blue', label='предсказанные данные')
ax.legend()
plt.show()

"""Чтобы минимизиовать влияние неуданого случаного разбиения на обучение/тест, проведем 5-кратную кросс-валидацию"""

# evaluate SVR using 5-fold crossvalidation
svrModel2 = SVR(C=1.0, epsilon=0.2)
scores = cross_validate(svrModel2,
                  dataScaled['x'].values.reshape(-1,1),
                  dataScaled['y'].values.reshape(-1,1),
                  scoring=('r2', 'neg_mean_squared_error'))

scores

print('CV MSE:',
      np.mean(-1*scores['test_neg_mean_squared_error']))
print('CV R2:',
      np.mean(scores['test_r2']))

"""На эффективность решения влияет выбор гиперпараметров модели. Можно попробовать подобрать параметры путем поиска "по решетке" и проверкой каждого набора через кросс-валидацию."""

# grid search for SVR parameters using CV
svrModel3=SVR()
parameters = {'C': [0.1, 0.5, 1., 1.5],
              'epsilon': [0.1, 0.25, 0.5, 0.75, 1]}
clf = GridSearchCV(svrModel3, parameters,
                   scoring=('neg_mean_squared_error'))
clf.fit(dataScaled['x'].values.reshape(-1,1),
                  dataScaled['y'].values.reshape(-1,1))

print('best parameters:', clf.best_params_)
print('best score:', -1*clf.best_score_)

# plot best model
xSVR = np.linspace(0, 1, 200)
ySVR = clf.best_estimator_.predict(xSVR.reshape(-1,1))

fig, ax = plt.subplots(figsize=(12,7))
ax.plot(xSVR,ySVR,color='red', label='Модель SVR')
ax.scatter(dataScaled['x'],dataScaled['y'], color='blue', label='данные')
ax.legend()
plt.show()

"""Вернем данные, полученные по модели к исходному масштабу"""

# reverse scaling
input = (data['x']- scaler1.data_min_[0]) / scaler1.data_range_[0]
prediction = clf.best_estimator_.predict(input.values.reshape(-1,1))
output = prediction * scaler1.data_range_[1] + scaler1.data_min_[1]
data['SVR'] = output
data.head(3)

# final plots
xSin=np.linspace(minX, maxX, 200)
ySin=np.sin(xSin)

fig, ax = plt.subplots(figsize=(12,7))
ax.plot(xSin,ySin,color='red', label='истинная зависимость')
ax.scatter(data['x'],data['y'], color='blue', label='данные')
ax.scatter(data['x'],data['SVR'], color='green', label='модель')
ax.plot(np.sort(data['x']),data['SVR'][np.argsort(data['x'])], color='green', label='модель')
ax.legend()
plt.show()

"""# Задача классификация

Сгенерируем тетовую задачу классификации с помощью разделяющей поверхности.

$ y =\frac{x}{4} sin(3 \cdot x) + 0.9 \cdot x  $
"""

volume = 50
xMin = 0
xMax = 5
attr1 = np.random.uniform(xMin, xMax, volume)
attr2 = np.random.uniform(xMin, xMax, volume)
classLabel = np.where((attr1/4*np.sin(3.*attr1)+0.9*attr1-attr2)<0,
                      0, 1)

data2 = pd.DataFrame({'attr1':attr1, 'attr2':attr2,
                      'label': classLabel})
data2.head()

x = np.linspace(0,5)
y = x/4*np.sin(3.*x)+0.9*x

fig, ax = plt.subplots(figsize=(8,8))
ax.plot(x,y,color='red')
ax.scatter(attr1, attr2, c=np.where(classLabel,'green','blue'))
ax.set_xlim(0,5)
ax.set_ylim(0,5)
ax.set_xlabel('attr1')
ax.set_ylabel('attr2')
plt.show()

data2.info()

"""Метки классов определены как целый тип, нужно поменять на категориальный."""

# convert labels from numbers into category
data2['label'] = data2['label'].astype('category')
data2.info()

data2['label'].describe()

data2['label'].value_counts()

data2['label'].value_counts().plot(kind='bar')

train, test = train_test_split(data2)

"""Применим алгоритм Наивный Байесовский классификатор (вариант с гауссовыми функциями)"""

gnb1 = GaussianNB()
gnb1.fit(train[['attr1','attr2']].values,
         train['label'].values)
predLabels = gnb1.predict(test[['attr1','attr2']])

"""Посчитаем меры качества, матрицу ошибок и напечатаем полный отчет."""

print('accuracy:',
      mtrx.accuracy_score(predLabels,test['label'])*100)
print('error:', (
    1-mtrx.accuracy_score(predLabels,test['label']))*100)
print('f1:',
      mtrx.f1_score(predLabels,test['label']))

mtrx.confusion_matrix(predLabels,test['label'])

print(mtrx.classification_report(predLabels,test['label']))

"""Нарисуем прогноз на тестовой выборке."""

test['prediction']=predLabels

fig, ax = plt.subplots(figsize=(6,6))
ax.plot(x,y,color='red')
ax.scatter(test['attr1'], test['attr2'],
           c=np.where(test['prediction'],'green','blue'))
ax.set_xlim(0,5)
ax.set_ylim(0,5)
ax.set_xlabel('attr1')
ax.set_ylabel('attr2')
plt.show()

"""*Примечаение. Для классификации также надо делать кросс-валидацию. Если алгоритм имеет параметры - подбирать.*

# Задача кластеризации

Сгенерируем 3 нормально распределенных и непересекающиеся группы данных.
"""

# [0,10]
x1=np.random.normal(3,0.7,20)
y1=np.random.normal(2,0.5,20)
label1=np.array([1]*20)
x2=np.random.normal(7,0.7,20)
y2=np.random.normal(4,0.5,20)
label2=np.array([2]*20)
x3=np.random.normal(4,1,20)
y3=np.random.normal(7,0.7,20)
label3=np.array([3]*20)

fig, ax = plt.subplots(figsize=(6,6))
ax.scatter(x1, y1)
ax.scatter(x2, y2)
ax.scatter(x3, y3)
ax.set_xlim(0,10)
ax.set_ylim(0,10)
plt.show()

"""Сохраним в датафрейм. Метки групп мы используем только для проверки для себя, алгоритм про них не будет знать."""

x=np.concatenate((x1,x2,x3))
y=np.concatenate((y1,y2,y3))
label=np.concatenate((label1,label2,label3))
data3=pd.DataFrame({'x':x,'y':y, 'label':label})
data3.head()

"""Построим дендрограмму (пример взят с сайта matplotlib),"""

def plot_dendrogram(model, **kwargs):
    # Create linkage matrix and then plot the dendrogram

    # create the counts of samples under each node
    counts = np.zeros(model.children_.shape[0])
    n_samples = len(model.labels_)
    for i, merge in enumerate(model.children_):
        current_count = 0
        for child_idx in merge:
            if child_idx < n_samples:
                current_count += 1  # leaf node
            else:
                current_count += counts[child_idx - n_samples]
        counts[i] = current_count

    linkage_matrix = np.column_stack([model.children_, model.distances_,
                                      counts]).astype(float)

    # Plot the corresponding dendrogram
    dendrogram(linkage_matrix, **kwargs)

model = AgglomerativeClustering(distance_threshold=0, n_clusters=None)
model = model.fit(data3[['x','y']])

plt.title('Hierarchical Clustering Dendrogram')
# plot the top three levels of the dendrogram
plot_dendrogram(model, truncate_mode='level', p=3)
plt.xlabel("Number of points in node (or index of point if no parenthesis).")
plt.show()

"""Решим задачу методом К-средних с инициализацией начальных положений кластеров k-means++."""

kmeans = KMeans(n_clusters=3, init='k-means++')
kmeans.fit(data3[['x','y']])

kmeans.cluster_centers_

fig, ax = plt.subplots(figsize=(6,6))
ax.scatter(x1, y1)
ax.scatter(x2, y2)
ax.scatter(x3, y3)
ax.scatter(kmeans.cluster_centers_[:,0],kmeans.cluster_centers_[:,1], color='red')
ax.set_xlim(0,10)
ax.set_ylim(0,10)
plt.show()

# прогноз меток кластеров
#Могут не совпадать с исходными. Важно чтобы в одной группе были одинаковые метки, какие именно - не важно, можно переназвать.
kmeans.labels_

# итоговое значение "инерции"
kmeans.inertia_

"""Проверим выбор числа кластов методом "локтя"
"""

inertia = []

for i in range(10):
  kmeans = KMeans(n_clusters=i+1, init='k-means++')
  kmeans.fit(data3[['x','y']])
  inertia.append(kmeans.inertia_)

fig, ax = plt.subplots(figsize=(10,6))
ax.plot(np.arange(1,11), inertia)
ax.scatter(np.arange(1,11), inertia)
plt.show()